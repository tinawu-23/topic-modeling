{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            NSF Award Title Non-phrases  index\n",
      "0     application applications basic premise data ba...      0\n",
      "1     algorithm implementation algorithms analysis a...      1\n",
      "2     assumption basic problems certain special case...      2\n",
      "3     active updating amount analysis analytical res...      3\n",
      "4     act act theory computer computer simulator com...      4\n",
      "5     artificial intelligence basic query languages ...      5\n",
      "6     abstract prosidic categories abstract prosodic...      6\n",
      "7      dynamical  processes  energy -processing  inf...      7\n",
      "8     ability analytically derived concepts approach...      8\n",
      "9     acquisition ambiguous examples classification ...      9\n",
      "10    advance applications artificial intelligence r...     10\n",
      "11    addition arm assembly capabilities complex rob...     11\n",
      "12    active experimentation additional knowledge ap...     12\n",
      "13    addition areas cad/cam commercial reality comp...     13\n",
      "14    ability addition analogical processing boilers...     14\n",
      "15    3-d structure algorithms analysis current stru...     15\n",
      "16    application attempt axiomatic treatment comput...     16\n",
      "17    abstract complexity theory applications applic...     17\n",
      "18    arbitrary curves arbitrary positions articulat...     18\n",
      "19    binary independence model computer systems doc...     19\n",
      "20    algorithm acts appproach biochemical propertie...     20\n",
      "21    acoustic invariants area automatic speech reco...     21\n",
      "22    automatic information retrieval automatic info...     22\n",
      "23    ability alternative meanings artificial intell...     23\n",
      "24    ambiguities application assumptions complete s...     24\n",
      "25    adaptive sensorimotor control addition advance...     25\n",
      "26    account algebraic models artificial intelligen...     26\n",
      "27    ad hoc methods ad hoc techniques applicability...     27\n",
      "28    august california community control costs deve...     28\n",
      "29    access application environment approach charac...     29\n",
      "...                                                 ...    ...\n",
      "6892  brain function brain imaging genomics brain im...   6892\n",
      "6893  ability abstraction algorithms amount anything...   6893\n",
      "6894  actionable behavior change suggestions actiona...   6894\n",
      "6895  alignment b c classroom observations classroom...   6895\n",
      "6896  accurate previewing adaptive refinement adapti...   6896\n",
      "6897  adaptive learning progressions anticipated dis...   6897\n",
      "6898  ability advanced robots algorithms algorithms ...   6898\n",
      "6899  adjacent characters algebra word problem algor...   6899\n",
      "6900  access accuracy advantage association behavior...   6900\n",
      "6901  addition approaches areas artifact analysis aw...   6901\n",
      "6902  ability adapt answers array basis case chs com...   6902\n",
      "6903  adults areas autonomous manipulation methods b...   6903\n",
      "6904  advanced machine learning applications algorit...   6904\n",
      "6905  agent agents american universities area assess...   6905\n",
      "6906  //ischools.org/the-iconference 21st century ac...   6906\n",
      "6907  //iui.acm.org/2017 20-30 presentation 200-300 ...   6907\n",
      "6908  //icad.org/icad2017/ academic program accommod...   6908\n",
      "6909  //cscw.acm.org/2017/index.php 1.5-day event ta...   6909\n",
      "6910  access application programming interfaces appr...   6910\n",
      "6911  academics agency leaders ai artificial intelli...   6911\n",
      "6912  //humanrobotinteraction.org/2017 3-minute over...   6912\n",
      "6913  area boston challenges collaborations computer...   6913\n",
      "6914  //www.nsf.gov/crcns a report area award center...   6914\n",
      "6915  aamas academia addition advances advice agent-...   6915\n",
      "6916  //www.fg2017.org/ abstracts activities applica...   6916\n",
      "6917  % increase //chi2017.acm.org/ 2-day long event...   6917\n",
      "6918  april award co-authors conference corbon coref...   6918\n",
      "6919  computational approaches context core correct ...   6919\n",
      "6920  //www.um.org/umap2017/ 30-minute window abilit...   6920\n",
      "6921  //iswc2017.semanticweb.org/ academia activitie...   6921\n",
      "\n",
      "[6922 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 946: expected 1 fields, saw 2\\nSkipping line 1957: expected 1 fields, saw 2\\nSkipping line 2826: expected 1 fields, saw 2\\nSkipping line 4376: expected 1 fields, saw 2\\nSkipping line 5810: expected 1 fields, saw 3\\nSkipping line 5811: expected 1 fields, saw 3\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('NSF_awardtopics.txt', error_bad_lines=False)\n",
    "data.columns = ['NSF Award Title Non-phrases']\n",
    "data['index'] = data.index\n",
    "documents = data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ywu6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >= 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['collision-free', 'motion', 'completeness', 'complex', 'ideas', 'computer', 'science', 'cornell', 'degrees', 'dr.', 'donald', 'error', 'detection', 'excellent', 'speaker', 'exceptional', 'work', 'freedom', 'influential', 'contribution', 'inspiring', 'graduate', 'advisor', 'kinodynamic', 'approximation', 'algorithms', 'master', 'mathematics', 'mit', 'new', 'graduate', 'course', 'optimal', 'robotic', 'motion', 'planning', 'planning', 'system', 'presidential', 'young', 'investigator', 'award', 'presidential', 'young', 'investigator', 'award', 'project', 'promise', 'recovery', 'research', 'rigorous', 'proof', 'robot', 'arms', 'robot', 'motion', 'planning', 'robotics', 'russian', 'literature', 'straightforward', 'manner', 'uncertain', 'domains', 'undergraduate', 'degree', 'work', 'yale', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['collis', 'free', 'motion', 'complet', 'complex', 'idea', 'scienc', 'cornel', 'degre', 'donald', 'error', 'detect', 'excel', 'speaker', 'except', 'work', 'freedom', 'influenti', 'contribut', 'inspir', 'graduat', 'advisor', 'kinodynam', 'approxim', 'algorithm', 'master', 'mathemat', 'mit', 'new', 'graduat', 'cours', 'optim', 'robot', 'motion', 'plan', 'plan', 'presidenti', 'young', 'investig', 'award', 'presidenti', 'young', 'investig', 'award', 'project', 'promis', 'recoveri', 'research', 'rigor', 'proof', 'robot', 'arm', 'robot', 'motion', 'plan', 'robot', 'russian', 'literatur', 'straightforward', 'manner', 'uncertain', 'domain', 'undergradu', 'degre', 'work', 'yale']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 250].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['NSF Award Title Non-phrases'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [applic, applic, basic, premis, data, base, su...\n",
       "1    [algorithm, implement, algorithm, analysi, app...\n",
       "2    [assumpt, basic, problem, certain, special, ca...\n",
       "3    [activ, updat, analysi, analyt, result, archiv...\n",
       "4    [act, act, theori, simul, teach, system, effec...\n",
       "5    [artifici, intellig, basic, queri, languag, sc...\n",
       "6    [abstract, prosid, categori, abstract, prosod,...\n",
       "7    [dynam, process, energi, process, inform, mech...\n",
       "8    [abil, analyt, deriv, concept, approach, artif...\n",
       "9    [acquisit, ambigu, exampl, classif, classif, d...\n",
       "Name: NSF Award Title Non-phrases, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 19 (\"studi\") appears 1 time.\n",
      "Word 22 (\"system\") appears 1 time.\n",
      "Word 24 (\"analysi\") appears 1 time.\n",
      "Word 72 (\"imag\") appears 3 time.\n",
      "Word 77 (\"local\") appears 1 time.\n",
      "Word 87 (\"recognit\") appears 1 time.\n",
      "Word 97 (\"stereo\") appears 1 time.\n",
      "Word 98 (\"structur\") appears 1 time.\n",
      "Word 103 (\"vision\") appears 1 time.\n",
      "Word 158 (\"theori\") appears 1 time.\n",
      "Word 178 (\"match\") appears 1 time.\n",
      "Word 234 (\"represent\") appears 1 time.\n",
      "Word 254 (\"biolog\") appears 1 time.\n",
      "Word 271 (\"energi\") appears 1 time.\n",
      "Word 343 (\"multipl\") appears 1 time.\n",
      "Word 403 (\"scale\") appears 1 time.\n",
      "Word 410 (\"set\") appears 1 time.\n",
      "Word 650 (\"tempor\") appears 1 time.\n",
      "Word 735 (\"grant\") appears 1 time.\n",
      "Word 819 (\"compress\") appears 1 time.\n",
      "Word 857 (\"textur\") appears 1 time.\n",
      "Word 1066 (\"cross\") appears 1 time.\n",
      "Word 1215 (\"filter\") appears 1 time.\n",
      "Word 1266 (\"discontinu\") appears 1 time.\n",
      "Word 1390 (\"transform\") appears 1 time.\n",
      "Word 1898 (\"discrimin\") appears 1 time.\n",
      "Word 1899 (\"wavelet\") appears 3 time.\n",
      "Word 1900 (\"zero\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_200 = bow_corpus[200]\n",
    "\n",
    "for i in range(len(bow_doc_200)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_200[i][0], \n",
    "                                                     dictionary[bow_doc_200[i][0]], \n",
    "                                                     bow_doc_200[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.1339125775909705),\n",
      " (1, 0.1386083656890697),\n",
      " (2, 0.15253811750038598),\n",
      " (3, 0.14035240208466113),\n",
      " (4, 0.04010163575335318),\n",
      " (5, 0.20298870938999058),\n",
      " (6, 0.24931968873366708),\n",
      " (7, 0.1795533691438295),\n",
      " (8, 0.14737521421206454),\n",
      " (9, 0.5120353116075402),\n",
      " (10, 0.20537297139559396),\n",
      " (11, 0.2936340106937988),\n",
      " (12, 0.08054858824528914),\n",
      " (13, 0.1364277375974575),\n",
      " (14, 0.3209511913532448),\n",
      " (15, 0.09377233559606106),\n",
      " (16, 0.28886412678393164),\n",
      " (17, 0.19580488634904608),\n",
      " (18, 0.016019650289156705),\n",
      " (19, 0.07540109644963205),\n",
      " (20, 0.2779899971764848),\n",
      " (21, 0.0989745687829497),\n",
      " (22, 0.13665896037909775)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.021*\"research\" + 0.013*\"behavior\" + 0.010*\"project\" + 0.009*\"design\" + 0.009*\"technolog\" + 0.008*\"comput\" + 0.007*\"new\" + 0.007*\"data\" + 0.007*\"student\" + 0.006*\"learn\"\n",
      "Topic: 1 \n",
      "Words: 0.022*\"research\" + 0.016*\"student\" + 0.013*\"project\" + 0.009*\"technolog\" + 0.009*\"new\" + 0.009*\"inform\" + 0.009*\"human\" + 0.008*\"imag\" + 0.008*\"field\" + 0.007*\"collabor\"\n",
      "Topic: 2 \n",
      "Words: 0.045*\"research\" + 0.029*\"student\" + 0.023*\"confer\" + 0.013*\"doctor\" + 0.012*\"particip\" + 0.012*\"workshop\" + 0.009*\"intern\" + 0.008*\"consortium\" + 0.008*\"field\" + 0.008*\"communiti\"\n",
      "Topic: 3 \n",
      "Words: 0.020*\"research\" + 0.013*\"data\" + 0.012*\"inform\" + 0.011*\"project\" + 0.009*\"collabor\" + 0.007*\"work\" + 0.007*\"knowledg\" + 0.007*\"web\" + 0.007*\"develop\" + 0.006*\"student\"\n",
      "Topic: 4 \n",
      "Words: 0.015*\"research\" + 0.010*\"model\" + 0.008*\"student\" + 0.008*\"project\" + 0.008*\"level\" + 0.008*\"inform\" + 0.007*\"energi\" + 0.007*\"agent\" + 0.007*\"video\" + 0.006*\"time\"\n",
      "Topic: 5 \n",
      "Words: 0.019*\"data\" + 0.017*\"algorithm\" + 0.014*\"research\" + 0.014*\"imag\" + 0.010*\"applic\" + 0.010*\"model\" + 0.010*\"structur\" + 0.009*\"new\" + 0.009*\"project\" + 0.008*\"analysi\"\n",
      "Topic: 6 \n",
      "Words: 0.018*\"network\" + 0.016*\"research\" + 0.013*\"data\" + 0.013*\"inform\" + 0.012*\"new\" + 0.010*\"model\" + 0.009*\"project\" + 0.009*\"social\" + 0.008*\"comput\" + 0.008*\"technolog\"\n",
      "Topic: 7 \n",
      "Words: 0.020*\"research\" + 0.013*\"model\" + 0.011*\"interact\" + 0.009*\"design\" + 0.009*\"project\" + 0.009*\"comput\" + 0.009*\"user\" + 0.008*\"human\" + 0.008*\"visual\" + 0.007*\"interfac\"\n",
      "Topic: 8 \n",
      "Words: 0.023*\"research\" + 0.015*\"social\" + 0.012*\"project\" + 0.011*\"languag\" + 0.009*\"new\" + 0.008*\"student\" + 0.008*\"comput\" + 0.008*\"data\" + 0.007*\"interact\" + 0.007*\"inform\"\n",
      "Topic: 9 \n",
      "Words: 0.017*\"human\" + 0.015*\"research\" + 0.014*\"model\" + 0.012*\"system\" + 0.010*\"robot\" + 0.010*\"object\" + 0.009*\"comput\" + 0.007*\"project\" + 0.007*\"recognit\" + 0.007*\"languag\"\n",
      "Topic: 10 \n",
      "Words: 0.021*\"research\" + 0.016*\"data\" + 0.012*\"system\" + 0.011*\"user\" + 0.010*\"applic\" + 0.010*\"model\" + 0.009*\"inform\" + 0.008*\"new\" + 0.007*\"time\" + 0.007*\"project\"\n",
      "Topic: 11 \n",
      "Words: 0.019*\"inform\" + 0.015*\"research\" + 0.012*\"project\" + 0.011*\"data\" + 0.009*\"new\" + 0.008*\"user\" + 0.008*\"model\" + 0.008*\"method\" + 0.007*\"analysi\" + 0.007*\"algorithm\"\n",
      "Topic: 12 \n",
      "Words: 0.032*\"robot\" + 0.014*\"research\" + 0.012*\"human\" + 0.010*\"control\" + 0.009*\"task\" + 0.009*\"system\" + 0.009*\"problem\" + 0.009*\"new\" + 0.008*\"plan\" + 0.008*\"design\"\n",
      "Topic: 13 \n",
      "Words: 0.022*\"data\" + 0.019*\"research\" + 0.015*\"network\" + 0.010*\"project\" + 0.010*\"inform\" + 0.009*\"model\" + 0.008*\"user\" + 0.007*\"new\" + 0.006*\"algorithm\" + 0.006*\"method\"\n",
      "Topic: 14 \n",
      "Words: 0.030*\"data\" + 0.023*\"research\" + 0.013*\"learn\" + 0.009*\"project\" + 0.009*\"scienc\" + 0.008*\"new\" + 0.008*\"model\" + 0.008*\"communiti\" + 0.007*\"imag\" + 0.007*\"student\"\n",
      "Topic: 15 \n",
      "Words: 0.044*\"data\" + 0.018*\"research\" + 0.012*\"model\" + 0.010*\"project\" + 0.008*\"applic\" + 0.007*\"scienc\" + 0.007*\"new\" + 0.007*\"analysi\" + 0.007*\"inform\" + 0.006*\"larg\"\n",
      "Topic: 16 \n",
      "Words: 0.012*\"research\" + 0.011*\"project\" + 0.011*\"databas\" + 0.010*\"data\" + 0.010*\"speech\" + 0.010*\"design\" + 0.010*\"model\" + 0.009*\"technolog\" + 0.008*\"inform\" + 0.008*\"human\"\n",
      "Topic: 17 \n",
      "Words: 0.015*\"research\" + 0.014*\"inform\" + 0.012*\"robot\" + 0.011*\"algorithm\" + 0.010*\"learn\" + 0.009*\"technolog\" + 0.008*\"project\" + 0.008*\"data\" + 0.008*\"environ\" + 0.007*\"problem\"\n",
      "Topic: 18 \n",
      "Words: 0.021*\"research\" + 0.012*\"algorithm\" + 0.011*\"data\" + 0.010*\"new\" + 0.009*\"learn\" + 0.008*\"project\" + 0.008*\"inform\" + 0.008*\"problem\" + 0.007*\"applic\" + 0.007*\"languag\"\n",
      "Topic: 19 \n",
      "Words: 0.020*\"visual\" + 0.017*\"human\" + 0.016*\"data\" + 0.013*\"research\" + 0.011*\"new\" + 0.010*\"algorithm\" + 0.009*\"project\" + 0.009*\"learn\" + 0.009*\"method\" + 0.008*\"task\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.003*\"data\" + 0.003*\"imag\" + 0.003*\"visual\" + 0.003*\"languag\" + 0.003*\"model\" + 0.003*\"learn\" + 0.002*\"speech\" + 0.002*\"network\" + 0.002*\"algorithm\" + 0.002*\"interact\"\n",
      "Topic: 1 \n",
      "Words: 0.003*\"confer\" + 0.003*\"data\" + 0.003*\"student\" + 0.003*\"network\" + 0.003*\"doctor\" + 0.002*\"languag\" + 0.002*\"robot\" + 0.002*\"learn\" + 0.002*\"system\" + 0.002*\"web\"\n",
      "Topic: 2 \n",
      "Words: 0.003*\"data\" + 0.003*\"imag\" + 0.002*\"databas\" + 0.002*\"robot\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"visual\" + 0.002*\"comput\" + 0.002*\"queri\" + 0.002*\"agent\"\n",
      "Topic: 3 \n",
      "Words: 0.004*\"data\" + 0.003*\"robot\" + 0.003*\"visual\" + 0.003*\"human\" + 0.003*\"databas\" + 0.003*\"queri\" + 0.002*\"learn\" + 0.002*\"user\" + 0.002*\"vision\" + 0.002*\"inform\"\n",
      "Topic: 4 \n",
      "Words: 0.002*\"data\" + 0.002*\"robot\" + 0.002*\"model\" + 0.002*\"human\" + 0.002*\"object\" + 0.002*\"agent\" + 0.002*\"network\" + 0.002*\"technolog\" + 0.002*\"user\" + 0.002*\"system\"\n",
      "Topic: 5 \n",
      "Words: 0.003*\"robot\" + 0.002*\"workflow\" + 0.002*\"data\" + 0.002*\"human\" + 0.002*\"model\" + 0.002*\"agent\" + 0.002*\"design\" + 0.002*\"user\" + 0.002*\"decis\" + 0.002*\"languag\"\n",
      "Topic: 6 \n",
      "Words: 0.003*\"robot\" + 0.003*\"data\" + 0.002*\"design\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"network\" + 0.002*\"user\" + 0.002*\"human\" + 0.002*\"interact\" + 0.002*\"imag\"\n",
      "Topic: 7 \n",
      "Words: 0.004*\"robot\" + 0.003*\"imag\" + 0.003*\"data\" + 0.003*\"network\" + 0.003*\"learn\" + 0.003*\"languag\" + 0.002*\"model\" + 0.002*\"social\" + 0.002*\"system\" + 0.002*\"problem\"\n",
      "Topic: 8 \n",
      "Words: 0.004*\"robot\" + 0.003*\"plan\" + 0.003*\"data\" + 0.003*\"algorithm\" + 0.002*\"environ\" + 0.002*\"system\" + 0.002*\"problem\" + 0.002*\"control\" + 0.002*\"network\" + 0.002*\"object\"\n",
      "Topic: 9 \n",
      "Words: 0.004*\"data\" + 0.003*\"databas\" + 0.003*\"imag\" + 0.002*\"inform\" + 0.002*\"model\" + 0.002*\"network\" + 0.002*\"algorithm\" + 0.002*\"structur\" + 0.002*\"robot\" + 0.002*\"comput\"\n",
      "Topic: 10 \n",
      "Words: 0.002*\"data\" + 0.002*\"inform\" + 0.002*\"databas\" + 0.002*\"visual\" + 0.002*\"model\" + 0.002*\"queri\" + 0.002*\"social\" + 0.002*\"user\" + 0.002*\"design\" + 0.002*\"system\"\n",
      "Topic: 11 \n",
      "Words: 0.004*\"confer\" + 0.003*\"doctor\" + 0.003*\"languag\" + 0.003*\"student\" + 0.003*\"robot\" + 0.003*\"consortium\" + 0.002*\"data\" + 0.002*\"learn\" + 0.002*\"social\" + 0.002*\"human\"\n",
      "Topic: 12 \n",
      "Words: 0.004*\"data\" + 0.003*\"imag\" + 0.003*\"learn\" + 0.003*\"languag\" + 0.002*\"databas\" + 0.002*\"visual\" + 0.002*\"model\" + 0.002*\"analysi\" + 0.002*\"inform\" + 0.002*\"scienc\"\n",
      "Topic: 13 \n",
      "Words: 0.004*\"robot\" + 0.003*\"confer\" + 0.003*\"student\" + 0.003*\"workshop\" + 0.002*\"human\" + 0.002*\"creativ\" + 0.002*\"design\" + 0.002*\"imag\" + 0.002*\"data\" + 0.002*\"particip\"\n",
      "Topic: 14 \n",
      "Words: 0.003*\"robot\" + 0.003*\"imag\" + 0.003*\"data\" + 0.002*\"social\" + 0.002*\"visual\" + 0.002*\"object\" + 0.002*\"system\" + 0.002*\"inform\" + 0.002*\"confer\" + 0.002*\"human\"\n",
      "Topic: 15 \n",
      "Words: 0.004*\"data\" + 0.003*\"robot\" + 0.002*\"model\" + 0.002*\"databas\" + 0.002*\"student\" + 0.002*\"algorithm\" + 0.002*\"design\" + 0.002*\"visual\" + 0.002*\"network\" + 0.002*\"comput\"\n",
      "Topic: 16 \n",
      "Words: 0.003*\"data\" + 0.003*\"confer\" + 0.003*\"workshop\" + 0.002*\"visual\" + 0.002*\"robot\" + 0.002*\"student\" + 0.002*\"learn\" + 0.002*\"interact\" + 0.002*\"human\" + 0.002*\"user\"\n",
      "Topic: 17 \n",
      "Words: 0.003*\"robot\" + 0.003*\"data\" + 0.003*\"learn\" + 0.002*\"human\" + 0.002*\"agent\" + 0.002*\"model\" + 0.002*\"languag\" + 0.002*\"imag\" + 0.002*\"vision\" + 0.002*\"problem\"\n",
      "Topic: 18 \n",
      "Words: 0.003*\"data\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"inform\" + 0.002*\"databas\" + 0.002*\"robot\" + 0.002*\"system\" + 0.002*\"design\" + 0.002*\"technolog\" + 0.002*\"machin\"\n",
      "Topic: 19 \n",
      "Words: 0.005*\"robot\" + 0.003*\"data\" + 0.003*\"behavior\" + 0.002*\"human\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"design\" + 0.002*\"object\" + 0.002*\"control\" + 0.002*\"plan\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.28191640973091125\t Topic: 0.032*\"robot\" + 0.014*\"research\" + 0.012*\"human\" + 0.010*\"control\" + 0.009*\"task\"\n",
      "Score: 0.2680550813674927\t Topic: 0.017*\"human\" + 0.015*\"research\" + 0.014*\"model\" + 0.012*\"system\" + 0.010*\"robot\"\n",
      "Score: 0.025001585483551025\t Topic: 0.022*\"research\" + 0.016*\"student\" + 0.013*\"project\" + 0.009*\"technolog\" + 0.009*\"new\"\n",
      "Score: 0.025001585483551025\t Topic: 0.015*\"research\" + 0.010*\"model\" + 0.008*\"student\" + 0.008*\"project\" + 0.008*\"level\"\n",
      "Score: 0.025001585483551025\t Topic: 0.019*\"data\" + 0.017*\"algorithm\" + 0.014*\"research\" + 0.014*\"imag\" + 0.010*\"applic\"\n",
      "Score: 0.025001585483551025\t Topic: 0.020*\"research\" + 0.013*\"model\" + 0.011*\"interact\" + 0.009*\"design\" + 0.009*\"project\"\n",
      "Score: 0.025001585483551025\t Topic: 0.015*\"research\" + 0.014*\"inform\" + 0.012*\"robot\" + 0.011*\"algorithm\" + 0.010*\"learn\"\n",
      "Score: 0.025001585483551025\t Topic: 0.021*\"research\" + 0.012*\"algorithm\" + 0.011*\"data\" + 0.010*\"new\" + 0.009*\"learn\"\n",
      "Score: 0.025001585483551025\t Topic: 0.020*\"visual\" + 0.017*\"human\" + 0.016*\"data\" + 0.013*\"research\" + 0.011*\"new\"\n",
      "Score: 0.025001583620905876\t Topic: 0.021*\"research\" + 0.013*\"behavior\" + 0.010*\"project\" + 0.009*\"design\" + 0.009*\"technolog\"\n",
      "Score: 0.025001583620905876\t Topic: 0.045*\"research\" + 0.029*\"student\" + 0.023*\"confer\" + 0.013*\"doctor\" + 0.012*\"particip\"\n",
      "Score: 0.025001583620905876\t Topic: 0.020*\"research\" + 0.013*\"data\" + 0.012*\"inform\" + 0.011*\"project\" + 0.009*\"collabor\"\n",
      "Score: 0.025001583620905876\t Topic: 0.018*\"network\" + 0.016*\"research\" + 0.013*\"data\" + 0.013*\"inform\" + 0.012*\"new\"\n",
      "Score: 0.025001583620905876\t Topic: 0.023*\"research\" + 0.015*\"social\" + 0.012*\"project\" + 0.011*\"languag\" + 0.009*\"new\"\n",
      "Score: 0.025001583620905876\t Topic: 0.021*\"research\" + 0.016*\"data\" + 0.012*\"system\" + 0.011*\"user\" + 0.010*\"applic\"\n",
      "Score: 0.025001583620905876\t Topic: 0.019*\"inform\" + 0.015*\"research\" + 0.012*\"project\" + 0.011*\"data\" + 0.009*\"new\"\n",
      "Score: 0.025001583620905876\t Topic: 0.022*\"data\" + 0.019*\"research\" + 0.015*\"network\" + 0.010*\"project\" + 0.010*\"inform\"\n",
      "Score: 0.025001583620905876\t Topic: 0.030*\"data\" + 0.023*\"research\" + 0.013*\"learn\" + 0.009*\"project\" + 0.009*\"scienc\"\n",
      "Score: 0.025001583620905876\t Topic: 0.044*\"data\" + 0.018*\"research\" + 0.012*\"model\" + 0.010*\"project\" + 0.008*\"applic\"\n",
      "Score: 0.025001583620905876\t Topic: 0.012*\"research\" + 0.011*\"project\" + 0.011*\"databas\" + 0.010*\"data\" + 0.010*\"speech\"\n"
     ]
    }
   ],
   "source": [
    "test_title = 'Computer Vision'\n",
    "bow_vector = dictionary.doc2bow(preprocess(test_title))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6833124756813049\t Topic: 0.003*\"data\" + 0.002*\"model\" + 0.002*\"learn\" + 0.002*\"inform\" + 0.002*\"databas\"\n",
      "Score: 0.016667766496539116\t Topic: 0.004*\"robot\" + 0.003*\"imag\" + 0.003*\"data\" + 0.003*\"network\" + 0.003*\"learn\"\n",
      "Score: 0.016667764633893967\t Topic: 0.003*\"data\" + 0.003*\"imag\" + 0.003*\"visual\" + 0.003*\"languag\" + 0.003*\"model\"\n",
      "Score: 0.016667764633893967\t Topic: 0.003*\"confer\" + 0.003*\"data\" + 0.003*\"student\" + 0.003*\"network\" + 0.003*\"doctor\"\n",
      "Score: 0.016667764633893967\t Topic: 0.003*\"data\" + 0.003*\"imag\" + 0.002*\"databas\" + 0.002*\"robot\" + 0.002*\"model\"\n",
      "Score: 0.016667764633893967\t Topic: 0.004*\"data\" + 0.003*\"robot\" + 0.003*\"visual\" + 0.003*\"human\" + 0.003*\"databas\"\n",
      "Score: 0.016667764633893967\t Topic: 0.002*\"data\" + 0.002*\"robot\" + 0.002*\"model\" + 0.002*\"human\" + 0.002*\"object\"\n",
      "Score: 0.016667764633893967\t Topic: 0.003*\"robot\" + 0.002*\"workflow\" + 0.002*\"data\" + 0.002*\"human\" + 0.002*\"model\"\n",
      "Score: 0.016667764633893967\t Topic: 0.003*\"robot\" + 0.003*\"data\" + 0.002*\"design\" + 0.002*\"model\" + 0.002*\"learn\"\n",
      "Score: 0.016667764633893967\t Topic: 0.004*\"robot\" + 0.003*\"plan\" + 0.003*\"data\" + 0.003*\"algorithm\" + 0.002*\"environ\"\n",
      "Score: 0.016667764633893967\t Topic: 0.004*\"data\" + 0.003*\"databas\" + 0.003*\"imag\" + 0.002*\"inform\" + 0.002*\"model\"\n",
      "Score: 0.016667764633893967\t Topic: 0.002*\"data\" + 0.002*\"inform\" + 0.002*\"databas\" + 0.002*\"visual\" + 0.002*\"model\"\n",
      "Score: 0.016667764633893967\t Topic: 0.004*\"confer\" + 0.003*\"doctor\" + 0.003*\"languag\" + 0.003*\"student\" + 0.003*\"robot\"\n",
      "Score: 0.016667764633893967\t Topic: 0.004*\"data\" + 0.003*\"imag\" + 0.003*\"learn\" + 0.003*\"languag\" + 0.002*\"databas\"\n",
      "Score: 0.016667764633893967\t Topic: 0.004*\"robot\" + 0.003*\"confer\" + 0.003*\"student\" + 0.003*\"workshop\" + 0.002*\"human\"\n",
      "Score: 0.016667764633893967\t Topic: 0.003*\"robot\" + 0.003*\"imag\" + 0.003*\"data\" + 0.002*\"social\" + 0.002*\"visual\"\n",
      "Score: 0.016667764633893967\t Topic: 0.004*\"data\" + 0.003*\"robot\" + 0.002*\"model\" + 0.002*\"databas\" + 0.002*\"student\"\n",
      "Score: 0.016667764633893967\t Topic: 0.003*\"data\" + 0.003*\"confer\" + 0.003*\"workshop\" + 0.002*\"visual\" + 0.002*\"robot\"\n",
      "Score: 0.016667764633893967\t Topic: 0.003*\"robot\" + 0.003*\"data\" + 0.003*\"learn\" + 0.002*\"human\" + 0.002*\"agent\"\n",
      "Score: 0.016667764633893967\t Topic: 0.005*\"robot\" + 0.003*\"data\" + 0.003*\"behavior\" + 0.002*\"human\" + 0.002*\"model\"\n"
     ]
    }
   ],
   "source": [
    "test_title = 'Machine Learning'\n",
    "bow_vector = dictionary.doc2bow(preprocess(test_title))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5249949097633362\t Topic: 0.017*\"human\" + 0.015*\"research\" + 0.014*\"model\" + 0.012*\"system\" + 0.010*\"robot\"\n",
      "Score: 0.02500026859343052\t Topic: 0.020*\"research\" + 0.013*\"model\" + 0.011*\"interact\" + 0.009*\"design\" + 0.009*\"project\"\n",
      "Score: 0.02500026673078537\t Topic: 0.021*\"research\" + 0.013*\"behavior\" + 0.010*\"project\" + 0.009*\"design\" + 0.009*\"technolog\"\n",
      "Score: 0.02500026673078537\t Topic: 0.022*\"research\" + 0.016*\"student\" + 0.013*\"project\" + 0.009*\"technolog\" + 0.009*\"new\"\n",
      "Score: 0.02500026673078537\t Topic: 0.045*\"research\" + 0.029*\"student\" + 0.023*\"confer\" + 0.013*\"doctor\" + 0.012*\"particip\"\n",
      "Score: 0.02500026673078537\t Topic: 0.020*\"research\" + 0.013*\"data\" + 0.012*\"inform\" + 0.011*\"project\" + 0.009*\"collabor\"\n",
      "Score: 0.02500026673078537\t Topic: 0.015*\"research\" + 0.010*\"model\" + 0.008*\"student\" + 0.008*\"project\" + 0.008*\"level\"\n",
      "Score: 0.02500026673078537\t Topic: 0.019*\"data\" + 0.017*\"algorithm\" + 0.014*\"research\" + 0.014*\"imag\" + 0.010*\"applic\"\n",
      "Score: 0.02500026673078537\t Topic: 0.018*\"network\" + 0.016*\"research\" + 0.013*\"data\" + 0.013*\"inform\" + 0.012*\"new\"\n",
      "Score: 0.02500026673078537\t Topic: 0.023*\"research\" + 0.015*\"social\" + 0.012*\"project\" + 0.011*\"languag\" + 0.009*\"new\"\n",
      "Score: 0.02500026673078537\t Topic: 0.021*\"research\" + 0.016*\"data\" + 0.012*\"system\" + 0.011*\"user\" + 0.010*\"applic\"\n",
      "Score: 0.02500026673078537\t Topic: 0.019*\"inform\" + 0.015*\"research\" + 0.012*\"project\" + 0.011*\"data\" + 0.009*\"new\"\n",
      "Score: 0.02500026673078537\t Topic: 0.032*\"robot\" + 0.014*\"research\" + 0.012*\"human\" + 0.010*\"control\" + 0.009*\"task\"\n",
      "Score: 0.02500026673078537\t Topic: 0.022*\"data\" + 0.019*\"research\" + 0.015*\"network\" + 0.010*\"project\" + 0.010*\"inform\"\n",
      "Score: 0.02500026673078537\t Topic: 0.030*\"data\" + 0.023*\"research\" + 0.013*\"learn\" + 0.009*\"project\" + 0.009*\"scienc\"\n",
      "Score: 0.02500026673078537\t Topic: 0.044*\"data\" + 0.018*\"research\" + 0.012*\"model\" + 0.010*\"project\" + 0.008*\"applic\"\n",
      "Score: 0.02500026673078537\t Topic: 0.012*\"research\" + 0.011*\"project\" + 0.011*\"databas\" + 0.010*\"data\" + 0.010*\"speech\"\n",
      "Score: 0.02500026673078537\t Topic: 0.015*\"research\" + 0.014*\"inform\" + 0.012*\"robot\" + 0.011*\"algorithm\" + 0.010*\"learn\"\n",
      "Score: 0.02500026673078537\t Topic: 0.021*\"research\" + 0.012*\"algorithm\" + 0.011*\"data\" + 0.010*\"new\" + 0.009*\"learn\"\n",
      "Score: 0.02500026673078537\t Topic: 0.020*\"visual\" + 0.017*\"human\" + 0.016*\"data\" + 0.013*\"research\" + 0.011*\"new\"\n"
     ]
    }
   ],
   "source": [
    "test_title = 'Models'\n",
    "bow_vector = dictionary.doc2bow(preprocess(test_title))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
