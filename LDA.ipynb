{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing doc\n",
    "\n",
    "def preprocessing():\n",
    "    # read data\n",
    "    # TODO: in the script that generates non-phrases, remove ',' in numbers\n",
    "    file = pd.read_csv('NSF_awardtopics.txt', error_bad_lines=False)\n",
    "    file.columns = ['NSF Award Title Non-phrases']\n",
    "    documents = file['NSF Award Title Non-phrases']\n",
    "    \n",
    "    word2id = {}\n",
    "    id2word = {}\n",
    "    docs = []\n",
    "    currentDocument = []\n",
    "    currentWordId = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        segList = gensim.utils.simple_preprocess(document)\n",
    "        for word in segList: \n",
    "            if len(word) >= 3 and word not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "                if word in word2id:\n",
    "                    currentDocument.append(word2id[word])\n",
    "                else:\n",
    "                    currentDocument.append(currentWordId)\n",
    "                    word2id[word] = currentWordId\n",
    "                    id2word[currentWordId] = word\n",
    "                    currentWordId += 1\n",
    "        docs.append(currentDocument)\n",
    "        currentDocument = []\n",
    "    return docs, word2id, id2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through each document & randomly assign each word in the document to a topic z\n",
    "# gives you the topic representation of all documents & word distributions of all topics, though not good ones\n",
    "\n",
    "def randomInitialize():\n",
    "    # d: document index, doc: document content\n",
    "    for d, doc in enumerate(docs): \n",
    "        zCurrentDoc = []   # topics for each word in the current document\n",
    "        for w in doc:  # w: each word in the document\n",
    "            pz = np.divide(np.multiply(ndz[d, :], nzw[:, w]), nz)\n",
    "            \n",
    "            # draw samples from a multinomial distribution; \n",
    "            # (n, pvals, size=None): n is num of experiments, pvals is probabilities of each of the p different outcomes\n",
    "            # randomly sample a topic for word w. z is the topic index \n",
    "            z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "            \n",
    "            zCurrentDoc.append(z)  # add this word's topic to zCurrentDoc\n",
    "            \n",
    "            # update variables\n",
    "            ndz[d, z] += 1\n",
    "            nzw[z, w] += 1\n",
    "            nz[z] += 1\n",
    "            \n",
    "        Z.append(zCurrentDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve by using gibbs sampling:\n",
    "# for each doc d, \n",
    "#    for each word w, \n",
    "#         for each topic z, compute two things: 1. p(topic z| doc d) 2. p(word w| topic t)\n",
    "#         we reassign w a new topic z with probability p(topic z| doc d) * p(word w| topic t) (p that topic t generated w)\n",
    "\n",
    "def gibbsSampling():\n",
    "    # regenerate topics for each word in each document\n",
    "    for d, doc in enumerate(docs):\n",
    "        for index, w in enumerate(doc):\n",
    "            z = Z[d][index]\n",
    "            # decrement the topic cnt for this word in this doc\n",
    "            ndz[d, z] -= 1\n",
    "            nzw[z, w] -= 1\n",
    "            nz[z] -= 1\n",
    "            \n",
    "            # recalculate the probability of each word w belonging to each topic z\n",
    "            pz = np.divide(np.multiply(ndz[d, :], nzw[:, w]), nz)\n",
    "            \n",
    "            # resample from the updated distribution\n",
    "            z = np.random.multinomial(1, pz / pz.sum()).argmax()\n",
    "            \n",
    "            Z[d][index] = z \n",
    "            \n",
    "            # update variables\n",
    "            ndz[d, z] += 1\n",
    "            nzw[z, w] += 1\n",
    "            nz[z] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 946: expected 1 fields, saw 2\\nSkipping line 1957: expected 1 fields, saw 2\\nSkipping line 2826: expected 1 fields, saw 2\\nSkipping line 4376: expected 1 fields, saw 2\\nSkipping line 5810: expected 1 fields, saw 3\\nSkipping line 5811: expected 1 fields, saw 3\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 completed.\n",
      "Iteration 2 completed.\n",
      "Iteration 3 completed.\n",
      "Iteration 4 completed.\n",
      "Iteration 5 completed.\n",
      "Iteration 6 completed.\n",
      "Iteration 7 completed.\n",
      "Iteration 8 completed.\n",
      "Iteration 9 completed.\n",
      "Iteration 10 completed.\n",
      "Iteration 11 completed.\n",
      "Iteration 12 completed.\n",
      "Iteration 13 completed.\n",
      "Iteration 14 completed.\n",
      "Iteration 15 completed.\n",
      "Iteration 16 completed.\n",
      "Iteration 17 completed.\n",
      "Iteration 18 completed.\n",
      "Iteration 19 completed.\n",
      "Iteration 20 completed.\n",
      "[['research', 'data', 'systems', 'students', 'applications', 'science', 'analysis', 'techniques', 'technology', 'project'], ['information', 'new', 'human', 'learning', 'project', 'algorithms', 'models', 'work', 'design', 'time']]\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "alpha = 5\n",
    "beta = 0.1\n",
    "\n",
    "# define iteration number\n",
    "iterationNum = 20\n",
    "\n",
    "# define how many topics we want to generate\n",
    "K = 10  # number of topics\n",
    "\n",
    "# preprocess\n",
    "docs, word2id, id2word = preprocessing()\n",
    "# print(docs)\n",
    "\n",
    "# variables\n",
    "Z = []  # list of lists. Z[i,j] means the topic index of [ith document, jth word]\n",
    "N = len(docs) # total number of documents\n",
    "M = len(word2id) # length of word list\n",
    "\n",
    "ndz = np.zeros([N, K]) + alpha  # ndz[i,z] means in the ith document, number of words generated by topic z\n",
    "nzw = np.zeros([K, M]) + beta   # nzw[z,w] means the number of word w generated by topic z\n",
    "nz = np.zeros([K]) + M * beta   # nz[z] means the number of each words generated by topic z\n",
    "\n",
    "# initialize\n",
    "randomInitialize()\n",
    "\n",
    "\n",
    "# realizing the pgm by gibbs sampling\n",
    "for i in range(0, iterationNum):\n",
    "    gibbsSampling()\n",
    "    print(\"Iteration {} completed.\".format(i+1))\n",
    "    \n",
    "# show results\n",
    "topicwords = []\n",
    "maxTopicWordsNum = 8  # let's say we want to show max top 10 words that contribute to each topic\n",
    "\n",
    "for z in range(0, K): # for each topic\n",
    "    ids = nzw[z, :].argsort()  # number of each word generated by topic z, ranked\n",
    "    topicword = []\n",
    "    for j in ids:\n",
    "        topicword.insert(0, id2word[j])  # in decrementing order\n",
    "    topicwords.append(topicword[0 : min(10, len(topicword))])\n",
    "    \n",
    "print(topicwords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
